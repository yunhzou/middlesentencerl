{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d29b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yunhengzou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/yunhengzou/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")          # classic Punkt models\n",
    "nltk.download(\"punkt_tab\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b44d8f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREV:\n",
      "Transformer models have revolutionized natural language processing. However, their quadratic time complexity on long sequences motivates efficient variants.\n",
      "\n",
      "TARGET:\n",
      "Sparse attention, kernel methods, and recurrent gating are popular directions. Recently, the Mamba architecture introduced state‑space gating for sequence modeling.\n",
      "\n",
      "AFTER:\n",
      "It achieves linear scaling while outperforming previous efficient transformers. We explore its applicability to protein‑fold prediction tasks.\n",
      "\n",
      "INDEX:\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# single_doc_middle_sentence.py\n",
    "# Python 3.11 – self‑contained helper for “middle **block** of sentences”\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Dict, List, Optional\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------ #\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Sentence‑tokenize a document (Punkt).\"\"\"\n",
    "    return nltk.tokenize.sent_tokenize(text, language=\"english\")\n",
    "\n",
    "\n",
    "def middle_sample(\n",
    "    text: str,\n",
    "    mid_idx: Optional[int] = None,   # start index of the middle block\n",
    "    mid_len: int = 1,                # how many “middle” sentences to predict\n",
    "    n_prev: int = 3,\n",
    "    n_next: int = 3,\n",
    ") -> Dict[str, str | int]:\n",
    "    \"\"\"\n",
    "    Return a training example with **multiple middle sentences**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "        prev   : str   – `n_prev` sentences before the middle block\n",
    "        target : str   – `mid_len` sentences to be predicted\n",
    "        after  : str   – `n_next` sentences after the block\n",
    "        index  : int   – sentence index of the *first* prev sentence\n",
    "                         (i.e. start of the whole chunk)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mid_idx : int | None\n",
    "        Starting sentence index of the block to predict.\n",
    "        • If None → choose the centred valid position.\n",
    "    mid_len : int\n",
    "        Number of sentences in the middle block (≥1).\n",
    "    n_prev, n_next : int\n",
    "        Context lengths before and after the middle block.\n",
    "    \"\"\"\n",
    "    if mid_len < 1:\n",
    "        raise ValueError(\"mid_len must be ≥ 1\")\n",
    "\n",
    "    sents = split_sentences(text)\n",
    "    total = len(sents)\n",
    "\n",
    "    need = n_prev + mid_len + n_next\n",
    "    if total < need:\n",
    "        raise ValueError(f\"doc too short: need ≥{need} sentences, got {total}\")\n",
    "\n",
    "    # establish valid range for the *start* index of the middle block\n",
    "    start_min = n_prev\n",
    "    start_max = total - mid_len - n_next\n",
    "\n",
    "    if mid_idx is None:  # centred start within allowed band\n",
    "        mid_idx = start_min + (start_max - start_min) // 2\n",
    "    else:\n",
    "        if not (start_min <= mid_idx <= start_max):\n",
    "            raise IndexError(\n",
    "                f\"mid_idx {mid_idx} invalid; valid range [{start_min}, {start_max}]\"\n",
    "            )\n",
    "\n",
    "    # build strings\n",
    "    prev_ctx = \" \".join(sents[mid_idx - n_prev : mid_idx])\n",
    "    target   = \" \".join(sents[mid_idx : mid_idx + mid_len])\n",
    "    after_ctx = \" \".join(sents[mid_idx + mid_len : mid_idx + mid_len + n_next])\n",
    "    index_prev_start = mid_idx - n_prev   # as requested\n",
    "\n",
    "    return {\n",
    "        \"prev\": prev_ctx,\n",
    "        \"target\": target,\n",
    "        \"after\": after_ctx,\n",
    "        \"index\": index_prev_start,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------- quick demo ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    DOC = (\n",
    "        \"Transformer models have revolutionized natural language processing. \"\n",
    "        \"However, their quadratic time complexity on long sequences motivates efficient variants. \"\n",
    "        \"Sparse attention, kernel methods, and recurrent gating are popular directions. \"\n",
    "        \"Recently, the Mamba architecture introduced state‑space gating for sequence modeling. \"\n",
    "        \"It achieves linear scaling while outperforming previous efficient transformers. \"\n",
    "        \"We explore its applicability to protein‑fold prediction tasks. \"\n",
    "        \"Initial results are promising and pave the way for future research.\"\n",
    "    )\n",
    "\n",
    "    sample = middle_sample(\n",
    "        DOC,\n",
    "        mid_idx=2,  # auto‑centre    \n",
    "        mid_len=2,     # predict two sentences\n",
    "        n_prev=2,\n",
    "        n_next=2,\n",
    "    )\n",
    "    for k, v in sample.items():\n",
    "        print(f\"{k.upper()}:\\n{v}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf1b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev Sentences: \n",
      " However, their quadratic time complexity on long sequences motivates efficient variants. Sparse attention, kernel methods, and recurrent gating are popular directions.\n",
      "After Sentences: \n",
      " It achieves linear scaling while outperforming previous efficient transformers.\n",
      "Target Sentence: \n",
      " Recently, the Mamba architecture introduced state‑space gating for sequence modeling.\n"
     ]
    }
   ],
   "source": [
    "print(\"Prev Sentences: \\n\", middle_sample(DOC, mid_idx=3, n_prev=2, n_next=1)[\"prev\"])\n",
    "print(\"After Sentences: \\n\", middle_sample(DOC, mid_idx=3, n_prev=2, n_next=1)[\"after\"])\n",
    "print(\"Target Sentence: \\n\", middle_sample(DOC, mid_idx=3, n_prev=2, n_next=1)[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f99f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab2722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5035a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "middlesentencerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
